"""
Sentiment Analysis â€” LSTM Inference Application
================================================
Production Streamlit interface for binary sentiment classification
using a pre-trained LSTM model with word-level tokenization.

Architecture : Embedding(128d) â†’ LSTM(128) â†’ Dense(1, sigmoid)
Tokenizer    : Keras Tokenizer (vocab=20k, OOV="<OOV>")
Input        : Raw text â†’ tokenized â†’ padded to 200 tokens (post)
Output       : POSITIVE / NEGATIVE with confidence indicator
"""

import streamlit as st
import tensorflow as tf
import pickle
from pathlib import Path
from tensorflow.keras.preprocessing.sequence import pad_sequences

# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# These MUST match training-time settings exactly.

MAX_LEN = 200
THRESHOLD = 0.5

BASE_DIR = Path(__file__).resolve().parent.parent
MODEL_PATH = BASE_DIR / "model" / "lstm_sentiment.h5"
TOKENIZER_PATH = BASE_DIR / "model" / "tokenizer.pkl"

EXAMPLES = {
    "Positive": "This product is amazing and exceeded all my expectations. Highly recommended!",
    "Negative": "Terrible quality. It broke after one day. Complete waste of money.",
    "Nuanced": "The camera is great but the battery life is really disappointing.",
}


# â”€â”€ Artifact Loading (cached) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_resource(show_spinner="Loading LSTM model...")
def load_model():
    """Load the trained Keras LSTM model from disk."""
    return tf.keras.models.load_model(str(MODEL_PATH))


@st.cache_resource(show_spinner="Loading tokenizer...")
def load_tokenizer():
    """Load the fitted Keras Tokenizer from disk."""
    with open(str(TOKENIZER_PATH), "rb") as f:
        return pickle.load(f)


# â”€â”€ Inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_inference(text: str, model, tokenizer) -> dict:
    """
    Full inference pipeline: text â†’ tokenize â†’ pad â†’ predict â†’ label.

    Returns
    -------
    dict with keys:
        label      : "POSITIVE" or "NEGATIVE"
        confidence : float in [0, 1] (distance from decision boundary, normalized)
        level      : qualitative descriptor ("Very High" / "High" / "Moderate" / "Low")
    """
    sequences = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(
        sequences, maxlen=MAX_LEN, padding="post", truncating="post"
    )
    prob = float(model.predict(padded, verbose=0)[0][0])

    label = "POSITIVE" if prob >= THRESHOLD else "NEGATIVE"

    # Confidence: normalized distance from the 0.5 decision boundary â†’ [0, 1]
    confidence = round(abs(prob - THRESHOLD) * 2, 4)

    if confidence >= 0.80:
        level = "Very High"
    elif confidence >= 0.60:
        level = "High"
    elif confidence >= 0.35:
        level = "Moderate"
    else:
        level = "Low"

    return {"label": label, "confidence": confidence, "level": level}


# â”€â”€ Result Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_result(result: dict, analyzed_text: str):
    """Render the prediction result card and confidence bar."""
    is_pos = result["label"] == "POSITIVE"
    icon = "ğŸ˜Š" if is_pos else "ğŸ˜"
    css = "result-pos" if is_pos else "result-neg"

    st.markdown(
        f"""
        <div class="result-card {css}">
            <div class="result-label">{icon} {result['label']}</div>
            <div class="result-meta">Confidence: {result['level']}</div>
        </div>
        """,
        unsafe_allow_html=True,
    )

    st.progress(result["confidence"], text=f"Model certainty â€” {result['level']}")

    with st.expander("ğŸ“„ Analyzed text"):
        st.write(analyzed_text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PAGE CONFIG & STYLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.set_page_config(
    page_title="LSTM Sentiment Analyzer",
    page_icon="ğŸ§ ",
    layout="centered",
    initial_sidebar_state="expanded",
)

st.markdown(
    """
    <style>
    .result-card {
        padding: 1.25rem 1.5rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .result-pos {
        background: linear-gradient(135deg, #d4edda, #c3e6cb);
        border-left: 5px solid #28a745;
    }
    .result-neg {
        background: linear-gradient(135deg, #f8d7da, #f5c6cb);
        border-left: 5px solid #dc3545;
    }
    .result-label {
        font-size: 1.5rem;
        font-weight: 700;
        margin-bottom: 0.2rem;
    }
    .result-meta {
        font-size: 0.92rem;
        color: #555;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SIDEBAR â€” Model Card
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with st.sidebar:
    st.header("ğŸ§  Model Card")

    st.markdown(
        """
| Component | Spec |
|:--|:--|
| Architecture | LSTM (Many-to-One) |
| Embedding | 128-d, trainable |
| LSTM Units | 128 |
| Dropout | 0.2 (+ recurrent) |
| Vocab Size | 20,000 |
| Sequence Length | 200 (post-padded) |
| Masking | Zero-masked embedding |
| Output | Dense(1, sigmoid) |
| Framework | TensorFlow / Keras |
"""
    )

    st.divider()
    st.subheader("âš™ï¸ Inference Pipeline")
    st.markdown(
        """
1. **Tokenize** â€” word-level, lowercased
2. **Encode** â€” integer sequences (`<OOV>` for unknowns)
3. **Pad / Truncate** â€” post, to 200 tokens
4. **Forward pass** â€” Embedding â†’ LSTM â†’ Dense
5. **Classify** â€” sigmoid â‰¥ 0.5 â†’ POSITIVE
"""
    )

    st.divider()
    st.caption("TensorFlow Â· Keras Â· Streamlit")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN CONTENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.title("ğŸ“ Sentiment Analysis")
st.markdown(
    "Classify text as **POSITIVE** or **NEGATIVE** using a pre-trained "
    "LSTM neural network trained on 240 k+ reviews."
)

# Load artifacts early (cached â€” runs once)
model = load_model()
tokenizer = load_tokenizer()

# â”€â”€ Session state for persisting results across reruns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "result" not in st.session_state:
    st.session_state.result = None
    st.session_state.analyzed_text = ""

# â”€â”€ Text input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
text_input = st.text_area(
    "Enter text to analyze:",
    height=130,
    placeholder="Type or paste a review, comment, or any text here...",
)

# â”€â”€ Analyze button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ğŸ” Analyze Sentiment", type="primary", use_container_width=True):
    if text_input and text_input.strip():
        with st.spinner("Running inference..."):
            st.session_state.result = run_inference(text_input.strip(), model, tokenizer)
            st.session_state.analyzed_text = text_input.strip()
    else:
        st.warning("Please enter some text to analyze.")

# â”€â”€ Quick examples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.divider()
st.markdown("##### ğŸ’¡ Quick Examples")

cols = st.columns(len(EXAMPLES))
for col, (name, text) in zip(cols, EXAMPLES.items()):
    with col:
        if st.button(name, key=f"ex_{name}", use_container_width=True, help=text):
            with st.spinner("Running inference..."):
                st.session_state.result = run_inference(text, model, tokenizer)
                st.session_state.analyzed_text = text

# â”€â”€ Result display (persists via session state) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.session_state.result is not None:
    st.divider()
    render_result(st.session_state.result, st.session_state.analyzed_text)
